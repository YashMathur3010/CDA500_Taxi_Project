{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.fft import fft\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists for 2023-01.\n",
      "Loading data for 2023-01...\n",
      "Total records: 3,066,766\n",
      "Valid records: 2,993,140\n",
      "Records dropped: 73,626 (2.40%)\n",
      "Successfully processed data for 2023-01.\n",
      "File already exists for 2023-02.\n",
      "Loading data for 2023-02...\n",
      "Total records: 2,913,955\n",
      "Valid records: 2,845,058\n",
      "Records dropped: 68,897 (2.36%)\n",
      "Successfully processed data for 2023-02.\n",
      "File already exists for 2023-03.\n",
      "Loading data for 2023-03...\n",
      "Total records: 3,403,766\n",
      "Valid records: 3,331,705\n",
      "Records dropped: 72,061 (2.12%)\n",
      "Successfully processed data for 2023-03.\n",
      "File already exists for 2023-04.\n",
      "Loading data for 2023-04...\n",
      "Total records: 3,288,250\n",
      "Valid records: 3,214,922\n",
      "Records dropped: 73,328 (2.23%)\n",
      "Successfully processed data for 2023-04.\n",
      "File already exists for 2023-05.\n",
      "Loading data for 2023-05...\n",
      "Total records: 3,513,649\n",
      "Valid records: 3,435,875\n",
      "Records dropped: 77,774 (2.21%)\n",
      "Successfully processed data for 2023-05.\n",
      "File already exists for 2023-06.\n",
      "Loading data for 2023-06...\n",
      "Total records: 3,307,234\n",
      "Valid records: 3,233,969\n",
      "Records dropped: 73,265 (2.22%)\n",
      "Successfully processed data for 2023-06.\n",
      "File already exists for 2023-07.\n",
      "Loading data for 2023-07...\n",
      "Total records: 2,907,108\n",
      "Valid records: 2,838,637\n",
      "Records dropped: 68,471 (2.36%)\n",
      "Successfully processed data for 2023-07.\n",
      "File already exists for 2023-08.\n",
      "Loading data for 2023-08...\n",
      "Total records: 2,824,209\n",
      "Valid records: 2,758,739\n",
      "Records dropped: 65,470 (2.32%)\n",
      "Successfully processed data for 2023-08.\n",
      "File already exists for 2023-09.\n",
      "Loading data for 2023-09...\n",
      "Total records: 2,846,722\n",
      "Valid records: 2,782,920\n",
      "Records dropped: 63,802 (2.24%)\n",
      "Successfully processed data for 2023-09.\n",
      "File already exists for 2023-10.\n",
      "Loading data for 2023-10...\n",
      "Total records: 3,522,285\n",
      "Valid records: 3,446,406\n",
      "Records dropped: 75,879 (2.15%)\n",
      "Successfully processed data for 2023-10.\n",
      "File already exists for 2023-11.\n",
      "Loading data for 2023-11...\n",
      "Total records: 3,339,715\n",
      "Valid records: 3,267,940\n",
      "Records dropped: 71,775 (2.15%)\n",
      "Successfully processed data for 2023-11.\n",
      "File already exists for 2023-12.\n",
      "Loading data for 2023-12...\n",
      "Total records: 3,376,567\n",
      "Valid records: 3,313,957\n",
      "Records dropped: 62,610 (1.85%)\n",
      "Successfully processed data for 2023-12.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Now you can import from src\n",
    "from src.data_utils import load_and_process_taxi_data\n",
    "rides = load_and_process_taxi_data(year=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pickup_hour  pickup_location_id  rides\n",
       "0 2023-01-01 00:00:00                   2      0\n",
       "1 2023-01-01 01:00:00                   2      0\n",
       "2 2023-01-01 02:00:00                   2      0\n",
       "3 2023-01-01 03:00:00                   2      0\n",
       "4 2023-01-01 04:00:00                   2      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_utils import transform_raw_data_into_ts_data\n",
    "\n",
    "ts_data = transform_raw_data_into_ts_data(rides)\n",
    "ts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2277600</td>\n",
       "      <td>2.277600e+06</td>\n",
       "      <td>2.277600e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2023-07-02 11:30:00.000000256</td>\n",
       "      <td>1.327231e+02</td>\n",
       "      <td>1.644857e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2023-04-02 05:45:00</td>\n",
       "      <td>6.675000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2023-07-02 11:30:00</td>\n",
       "      <td>1.335000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-10-01 17:15:00</td>\n",
       "      <td>1.982500e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-12-31 23:00:00</td>\n",
       "      <td>2.630000e+02</td>\n",
       "      <td>9.620000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.587973e+01</td>\n",
       "      <td>5.091858e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         pickup_hour  pickup_location_id         rides\n",
       "count                        2277600        2.277600e+06  2.277600e+06\n",
       "mean   2023-07-02 11:30:00.000000256        1.327231e+02  1.644857e+01\n",
       "min              2023-01-01 00:00:00        2.000000e+00  0.000000e+00\n",
       "25%              2023-04-02 05:45:00        6.675000e+01  0.000000e+00\n",
       "50%              2023-07-02 11:30:00        1.335000e+02  0.000000e+00\n",
       "75%              2023-10-01 17:15:00        1.982500e+02  2.000000e+00\n",
       "max              2023-12-31 23:00:00        2.630000e+02  9.620000e+02\n",
       "std                              NaN        7.587973e+01  5.091858e+01"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ts_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import TRANSFORMED_DATA_DIR\n",
    "df = pd.read_parquet(TRANSFORMED_DATA_DIR / \"tabular_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55900, 674)\n",
      "(55900,)\n",
      "(31720, 674)\n",
      "(31720,)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from src.data_utils import split_time_series_data\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_time_series_data(\n",
    "    df,\n",
    "    cutoff_date=datetime(2023, 9, 1, 0, 0, 0),\n",
    "    target_column=\"target\"\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fft_features(time_series):\n",
    "    # Extract numerical values from the time series\n",
    "    numerical_values = time_series.astype(float).values\n",
    "    fft_result = fft(numerical_values)\n",
    "    fft_magnitude = np.abs(fft_result)\n",
    "    return fft_magnitude[:len(fft_magnitude)//2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot cast DatetimeArray to dtype float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m fft_features = \u001b[43mextract_fft_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mextract_fft_features\u001b[39m\u001b[34m(time_series)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_fft_features\u001b[39m(time_series):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Extract numerical values from the time series\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     numerical_values = \u001b[43mtime_series\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m.values\n\u001b[32m      4\u001b[39m     fft_result = fft(numerical_values)\n\u001b[32m      5\u001b[39m     fft_magnitude = np.abs(fft_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/taxi_project/lib/python3.11/site-packages/pandas/core/generic.py:6534\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6530\u001b[39m     results = [ser.astype(dtype, copy=copy) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()]\n\u001b[32m   6532\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6533\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6534\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6535\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/taxi_project/lib/python3.11/site-packages/pandas/core/internals/managers.py:414\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    412\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/taxi_project/lib/python3.11/site-packages/pandas/core/internals/managers.py:354\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    357\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/taxi_project/lib/python3.11/site-packages/pandas/core/internals/blocks.py:616\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow)\u001b[39m\n\u001b[32m    596\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[32m    598\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    612\u001b[39m \u001b[33;03mBlock\u001b[39;00m\n\u001b[32m    613\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    614\u001b[39m values = \u001b[38;5;28mself\u001b[39m.values\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    618\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    620\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/taxi_project/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:238\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    235\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/taxi_project/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:180\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np.ndarray):\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# i.e. ExtensionArray\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     values = \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    183\u001b[39m     values = _astype_nansafe(values, dtype, copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/taxi_project/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:727\u001b[39m, in \u001b[36mDatetimeArray.astype\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, PeriodDtype):\n\u001b[32m    726\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_period(freq=dtype.freq)\n\u001b[32m--> \u001b[39m\u001b[32m727\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDatetimeLikeArrayMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/taxi_project/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:489\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.astype\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmM\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype != dtype) \u001b[38;5;129;01mor\u001b[39;00m dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    486\u001b[39m     \u001b[38;5;66;03m# disallow conversion between datetime/timedelta,\u001b[39;00m\n\u001b[32m    487\u001b[39m     \u001b[38;5;66;03m# and conversions for any datetimelike to float\u001b[39;00m\n\u001b[32m    488\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot cast \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(\u001b[38;5;28mself\u001b[39m, dtype=dtype)\n",
      "\u001b[31mTypeError\u001b[39m: Cannot cast DatetimeArray to dtype float64"
     ]
    }
   ],
   "source": [
    "fft_features = extract_fft_features(ts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Model definitions\n",
    "def fit_arma(data, order):\n",
    "    model = ARIMA(data, order=(order[0], 0, order[1]))\n",
    "    results = model.fit()\n",
    "    return results\n",
    "\n",
    "def fit_arima(data, order):\n",
    "    model = ARIMA(data, order=order)\n",
    "    results = model.fit()\n",
    "    return results\n",
    "\n",
    "def fit_prophet(data):\n",
    "    df = pd.DataFrame({'ds': data.index, 'y': data.values})\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    return model\n",
    "\n",
    "# Main pipeline\n",
    "def train_and_evaluate_models(data):\n",
    "    experiment = mlflow_setup()\n",
    "    fft_features = extract_fft_features(data)\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment.id):\n",
    "        mlflow.log_param(\"data_shape\", data.shape)\n",
    "        mlflow.log_param(\"fft_features_shape\", fft_features.shape)\n",
    "        \n",
    "        # ARMA\n",
    "        logger.info(\"Fitting ARMA model...\")\n",
    "        arma_model = fit_arma(data, order=(2, 1))\n",
    "        arma_predictions = arma_model.forecast(steps=len(data))\n",
    "        arma_mse = mean_squared_error(data, arma_predictions)\n",
    "        mlflow.log_metric(\"arma_mse\", arma_mse)\n",
    "        mlflow.log_metric(\"arma_aic\", arma_model.aic)\n",
    "        mlflow.sklearn.log_model(arma_model, \"arma_model\")\n",
    "        \n",
    "        # ARIMA\n",
    "        logger.info(\"Fitting ARIMA model...\")\n",
    "        arima_model = fit_arima(data, order=(1, 1, 1))\n",
    "        arima_predictions = arima_model.forecast(steps=len(data))\n",
    "        arima_mse = mean_squared_error(data, arima_predictions)\n",
    "        mlflow.log_metric(\"arima_mse\", arima_mse)\n",
    "        mlflow.log_metric(\"arima_aic\", arima_model.aic)\n",
    "        mlflow.sklearn.log_model(arima_model, \"arima_model\")\n",
    "        \n",
    "        # Prophet\n",
    "        logger.info(\"Fitting Prophet model...\")\n",
    "        prophet_model = fit_prophet(data)\n",
    "        future_dates = prophet_model.make_future_dataframe(periods=len(data))\n",
    "        prophet_forecast = prophet_model.predict(future_dates)\n",
    "        prophet_predictions = prophet_forecast['yhat'][-len(data):]\n",
    "        prophet_mse = mean_squared_error(data, prophet_predictions)\n",
    "        mlflow.log_metric(\"prophet_mse\", prophet_mse)\n",
    "        mlflow.pyfunc.log_model(\"prophet_model\", python_model=prophet_model)\n",
    "        \n",
    "        # Log FFT features\n",
    "        mlflow.log_param(\"fft_features\", fft_features.tolist())\n",
    "        \n",
    "        logger.info(\"Model training and evaluation completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
