{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import lightgbm as lgb\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.model_selection import train_test_split #Removed\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import cross_val_score #Removed\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.config import TRANSFORMED_DATA_DIR\n",
    "from src.data_utils import split_time_series_data\n",
    "from src.experiment_utils import set_mlflow_tracking, log_model_to_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time_series_data_cutoff(\n",
    "    df: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    cutoff_date: str\n",
    ") -> tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Splits a time series DataFrame into training and testing sets based on a cutoff date.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the time series data.\n",
    "        target_column (str): The name of the target column to separate from the features.\n",
    "        cutoff_date (str):  Date string (\"YYYY-MM-DD HH:MM:SS\")\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "        - X_train (pd.DataFrame): Training features.\n",
    "        - y_train (pd.Series): Training target values.\n",
    "        - X_test (pd.DataFrame): Testing features.\n",
    "        - y_test (pd.Series): Testing target values.\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by date\n",
    "    df_sorted = df.sort_values(\"pickup_hour\")\n",
    "\n",
    "    # Convert the cutoff date string to a datetime object\n",
    "    cutoff_datetime = pd.to_datetime(cutoff_date)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_data = df_sorted[df_sorted[\"pickup_hour\"] < cutoff_datetime].reset_index(drop=True)\n",
    "    test_data = df_sorted[df_sorted[\"pickup_hour\"] >= cutoff_datetime].reset_index(drop=True)\n",
    "\n",
    "    # Separate features (X) and target (y) for both sets\n",
    "    X_train = train_data.drop(columns=[target_column])\n",
    "    y_train = train_data[target_column]\n",
    "    X_test = test_data.drop(columns=[target_column])\n",
    "    y_test = test_data[target_column]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "df = pd.read_parquet(TRANSFORMED_DATA_DIR / \"tabular_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (8060, 674)\n",
      "Test set shape: (79560, 674)\n"
     ]
    }
   ],
   "source": [
    "# Define the cutoff date\n",
    "CUTOFF_DATE = \"2023-03-01 00:00:00\"\n",
    "\n",
    "# Split the data using the new function\n",
    "X_train, y_train, X_test, y_test = split_time_series_data_cutoff(df, target_column=\"target\", cutoff_date=CUTOFF_DATE)\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "def average_rides_last_4_weeks(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    last_4_weeks_columns = [\n",
    "        f\"rides_t-{7*24}\", # 1 week ago\n",
    "        f\"rides_t-{14*24}\", # 2 weeks ago\n",
    "        f\"rides_t-{21*24}\", # 3 weeks ago\n",
    "        f\"rides_t-{28*24}\" # 4 weeks ago\n",
    "    ]\n",
    "\n",
    "    # Ensure the required columns exist in the test DataFrame\n",
    "    for col in last_4_weeks_columns:\n",
    "        if col not in X.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    # Calculate the average of the last 4 weeks\n",
    "    X[\"average_rides_last_4_weeks\"] = X[last_4_weeks_columns].mean(axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "add_feature_average_rides_last_4_weeks = FunctionTransformer(\n",
    "    average_rides_last_4_weeks, validate=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 134340\n",
      "[LightGBM] [Info] Number of data points in the train set: 8060, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 10.315261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 134340\n",
      "[LightGBM] [Info] Number of data points in the train set: 8060, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 10.315261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 134340\n",
      "[LightGBM] [Info] Number of data points in the train set: 8060, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 10.315261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 134340\n",
      "[LightGBM] [Info] Number of data points in the train set: 8060, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 10.315261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 134340\n",
      "[LightGBM] [Info] Number of data points in the train set: 8060, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 10.315261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 134340\n",
      "[LightGBM] [Info] Number of data points in the train set: 8060, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 10.315261\n"
     ]
    }
   ],
   "source": [
    "class TemporalFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        X_[\"hour\"] = X_[\"pickup_hour\"].dt.hour\n",
    "        X_[\"day_of_week\"] = X_[\"pickup_hour\"].dt.dayofweek\n",
    "\n",
    "        return X_.drop(columns=[\"pickup_hour\", \"pickup_location_id\"])\n",
    "\n",
    "add_temporal_features = TemporalFeatureEngineer()\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    add_feature_average_rides_last_4_weeks,\n",
    "    add_temporal_features,\n",
    "    lgb.LGBMRegressor()\n",
    ")\n",
    "\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5, 0.65, 0.75, 0.85, 0.95, 1]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = pipeline.set_params(lgbmregressor__learning_rate=lr)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "print(f\"Test mae: {test_mae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
